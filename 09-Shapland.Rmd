# Using the ODP Bootstrap Model: A Practitioner's Guide - Shapland

[Notations](#shapland-notations) are similar to [Venter](#venter-def)

* See Table \@ref(tab:odp-glm-para-tri) for sample triangle layout

***Model Parameters***:

* Mean \@ref(eq:odp-mean)

* Variance \@ref(eq:odp-var) and [dispersion factor](#odp-dispersion)

    * Need [residual](#odp-res): unscaled \@ref(eq:odp-pear-res), scaled \@ref(eq:odp-scaled-res), standardized \@ref(eq:odp-std-res)

* Parameterize with [GLM](#ODP-GLM-fit) or [simplified (ODP)](#simplified-glm) (*Note the condition required for ODP*)

$\star$ [***Simulation procedure***](#odp-boot-sim)

* Parameter variance \@ref(eq:odp-sim-loss)

* Process variance  \@ref(eq:proc-var-sim-shapland)

* [Other variation](#bootstrap-variation-shap) of the bootstrap (*e.g. incurred, BF, Cape Cod*)

* [Pros and Cons of GLM](#GLM-bootstrap) 

* [GLM vs ODP](#odp-vs-glm-shap) bootstrap

$\star$ ***Practical issues***:

1. $\star$ [Negative Incremental Values](#prac-issue-1)

    * During fitting: \@ref(eq:method-1-inc-neg-fit) and \@ref(eq:method-2-inc-neg-fit)
    
    * During simulation: \@ref(eq:gamma-adj-odp)

2. [Non‐Zero Sum of Residuals](#prac-issue-2)

3. [Using an L-year Weighted Average](#prac-issue-3)

4. [Missing Values](#prac-issue-4)

5. [Outliers](#prac-issue-5)

6. $\star$ [Heteroscedasticity](#prac-issue-6)

    * [Stratified sampling](#stratified-sample)
    
    * [Hetero adjustment to residuals](#hetero-adj-res)
    
    * [Non-constant scale parameter](#non-constant-scale-para)

7. [Heteroecthesious Data](#prac-issue-7)

8. [Exposure Adjustment](#prac-issue-8)

9. [Tail Factors](#prac-issue-9)

10. [Fitting a Distribution to ODP Bootstrap Residuals](#prac-issue-10)

***5 diagnostics***

1. $\star$ [Residual graphs](#odp-res-diag)

2. [Normality test](#odp-norm-test)

3. [Outliers](#odp-diag-outlier)

4. [Parameter adjustment](#odp-diag-para)

5. $\star$ [Model results](#odp-diag-results-review)

[Multiple models](#shapland-multi-mod)

[Model testing](#shapland-testing)

## Introduction

Paper focus on **over-dispersed Poisson** (ODP) **bootstrap**

* *Incremental losses* are modeled as ODP random variable

* Goal is to generate a distribution of possible outcomes

```{block, type='rmdtip'}
*Just FYI, not important for exam*

Other papers on bootstrap

* Statistics: Bradley Efron (1979)

* Actuarial: England & Verrall (1999; 2002), Pinheiro, et al. (2003), Kirschner, et al. (2008)
```

Practical **motivation** for modeling loss distribution

* Definition of actuarial estimate in ASOP 43 can be based on a first moment from a distribution

    * While ASOP 36 (SAO) focus on deterministic point estimates

* SEC is looking for more information on reserving risk in the 10-K

* Rating agencies are building dynamic risk models and welcome actuary input

* Companies that use dynamic risk models for internal risk management need unpaid claim distributions

* SII and IFAS are moving towards unpaid claim distribution

***Advantages*** of bootstrap

* Generates a distribution of the estimate of unpaid claims

* Can be tailored to [statistical features](#odp-diagnostics) of our data 

* Reflects that loss dist^n^ are usually skewed to the right

***Disadvantages*** of bootstrap

* Takes more time to create, but okay once set up

### Stochastic vs Static Model

ODP bootstrap is a specific form of GLM

**Benefit** of GLM: It can be specifically tailored to the statistical features found in the data

* Contrast with algorithms that force the data to be fit to a static model (fig. \@ref(fig:stoc-vs-stat))

```{r stoc-vs-stat, echo = FALSE, out.width='50%', fig.show='hold', fig.cap='Stochastic vs Static Model Diagram'}
knitr::include_graphics(c('figures/Exam-7-Notes-14.png',
                          'figures/Exam-7-Notes-15.png'))
```

```{block, type='rmdtip'}
*Just FYI, not important for exam*

Some authors define a model as having a **defined structure** and **error distribution**, so under this more restrictive definition bootstrapping would be considered to be a *method* or *algorithm* 

However, using a less restrictive definition of a model as an **algorithm that produces a distribution**, bootstrapping would be defined as a model
```

## Notations {#shapland-notations}

```{definition}
Same notations as [Venter](#venter-def) for $n \times n$ triangle

* $w$: Accident (exposure) year

* $d$: Development age

* $q(w,d)$: incremental loss for AY $w$ from age $d-1$ to $d$

* $c(w,d)$: cumulative loss

* $F(d)$: Incremental LDF from age $d$ to $d+1$

* $f(d)$: $F(d) - 1$, for forcasting incremental losses

* $G(w)$: Factor relating to accdient (exposure) year $w$; ultimate gross level

* $h(k)$: Factor relating to the diagonal $k$ along which $w+d$ is constant
```

Table: (\#tab:odp-glm-para-tri) Incremental triangle with corresponding row and column parameters

|   | - | $\beta_2$ | $\beta_3$ |
|:------:|:--------:|:--------:|:--------:|
| $\alpha_1$ | $q(1,1)$ | $q(1,2)$ | $q(1,3)$ |  
| $\alpha_2$ | $q(2,1)$ | $q(2,2)$ |          |
| $\alpha_3$ | $q(3,1)$ |          |          |

**Chainladder assumptions:**

1. LDFs are the same for each row

    $F(w,d) = F(d)$

2. Each AY has a parameter representing it's level

    e.g. CL project based on level of *losses to date*

## Bootstrap Model

**Benefits** of the bootstrap model:

* Allows us to estimate the distribution with very little data

* We don't have to make any assumptions about the underlying distribution (non-parametric)

    * The **ODP** part is the error distribution
    
ODP bootstrap models:

* Incremental claims directly as the response

* With the same linear predictor as Kremer (1982)

* Using a GLM with log-link function and an ODP Poisson error

* Where a specific form of this model is identical to the volume weighted chain ladder

* Using bootstrap (sampling residuals with replacement) to estimate the distribution of point estimates

    (Instead of simulating from a multivariate normal for a GLM)
    
### GLM Parameters {#odp-glm-para}

Mean and variance for each $q(w,d)$ in the triangle (per table \@ref(tab:odp-glm-para-tri))

#### Mean and log-mean for $q(w,d)$

\begin{equation}
  \mathrm{E}[q(w,d)] = m_{wd} = \exp \left [\alpha_w + \sum_{i=2}^d \beta_i \right] \:\: : \: \: w \in [2, n]
  (\#eq:odp-mean)
\end{equation}

\begin{equation}
  \ln \left( \mathrm{E}[q(w,d)] \right) = \ln(m_{w,d}) = \eta_{w,d} = \alpha_w + \sum_{i=2}^d \beta_i \:\: : \: \: w \in [2, n]
  (\#eq:odp-log-mean)
\end{equation}

```{remark}


* $\alpha$'s are the individual level parameters

* $\beta$'s adjust for the development trends after the first development period

    * We don't use $\beta_1$ which effectively means $\beta_1 = 0$

* $\alpha_i$ and $\beta_j$ are selected to minimize error between $\ln(actual) - \ln(forecast)$
```

Equivalence for using [Venter](#venter-def) notation:

* $h(w) = e^{\alpha}$

* $f(d) = e^{\sum \beta}$

#### Variance for $q(w,d)$

\begin{equation}
  \mathrm{Var}[q(w,d)] = \phi m_{wd}^z
  (\#eq:odp-var)
\end{equation}

* $\phi$: [Dispersion factor](#odp-dispersion)
    
    * Scale factor estimated as part of the fitting procedure while setting the variance proportional to the mean
    
    * Estimated from the residuals
    
* $z$: Error distribution

    * Paper focus on $z = 1$ for Over Dispersed Poisson (ODP)
    
    * Specifies the whole mean-variance relationship (not only the first 2 moments)

Table: (\#tab:odp-error-dist) Distribution with corresponding $z$

| $z$ | Distribution     |
| :-: | :---------------: |
| 0 | Normal           |
| 1 | Poisson          |
| 2 | Gamma            |
| 3 | Inverse Gaussian |

### Fitted Triangle {#fitted-tri-odp}

We can fit the $\alpha$'s and $\beta$'s defined [above](#odp-glm-para) using the [*GLM framework*](#ODP-GLM-fit), or the [*simplified GLM*](#simplified-glm) method

#### Parameterize with GLM Framework {#ODP-GLM-fit}

Start with a $3 \times 3$ incremental triangle

Table: (\#tab:odp-glm-1) $3\times 3$ incremental triangle:

|    w/d    |     1    |      2   |     3    |
|:------:|:--------:|:--------:|:--------:|
| **1**  | $q(1,1)$ | $q(1,2)$ | $q(1,3)$ |  
| **2**  | $q(2,1)$ | $q(2,2)$ |          |
| **3**  | $q(3,1)$ |          |          |

Log transform of the triangle

Table: (\#tab:odp-glm-2) $3\times 3$ log incremental triangle:

|   w/d     |     1    |      2   |     3    |
|:------:|:--------:|:--------:|:--------:|
| **1**  | $\ln[q(1,1)]$ | $\ln[q(1,2)]$ | $\ln[q(1,3)]$ |  
| **2**  | $\ln[q(2,1)]$ | $\ln[q(2,2)]$ |          |
| **3**  | $\ln[q(3,1)]$ |          |          |


Create a system of equations based on equation \@ref(eq:odp-log-mean)

\begin{equation}
\begin{split}
  \ln[q(1,1)] &= 1\alpha_1 + 0\alpha_2 + 0\alpha_3 + 0\beta_2 + 0\beta_3 \\
  \ln[q(2,1)] &= 0\alpha_1 + 1\alpha_2 + 0\alpha_3 + 0\beta_2 + 0\beta_3 \\
  \ln[q(3,1)] &= 0\alpha_1 + 0\alpha_2 + 1\alpha_3 + 0\beta_2 + 0\beta_3 \\
  \ln[q(1,2)] &= 1\alpha_1 + 0\alpha_2 + 0\alpha_3 + 1\beta_2 + 0\beta_3 \\
  \ln[q(2,2)] &= 0\alpha_1 + 1\alpha_2 + 0\alpha_3 + 1\beta_2 + 0\beta_3 \\
  \ln[q(1,3)] &= 0\alpha_1 + 0\alpha_2 + 1\alpha_3 + 1\beta_2 + 1\beta_3 \\
\end{split}
(\#eq:odp-sys-eq)
\end{equation}

Express the above in matrix form

\begin{equation}
\begin{array}{ccccc}
  \mathbf{Y} & = & \mathbf{X} &\times & \mathbf{A} \\
  & & \alpha_1 \:\:\: \alpha_2 \:\:\: \alpha_3 \:\:\: \beta_2 \:\:\: \beta_3 & &\\
  \begin{bmatrix}
    ln[q(1,1)] \\
    ln[q(2,1)] \\
    ln[q(3,1)] \\
    ln[q(1,2)] \\
    ln[q(2,2)] \\
    ln[q(1,3)] \\
  \end{bmatrix}
    & =
      &
      \begin{bmatrix}
        1 & - & - & - & - \\
        - & 1 & - & - & - \\
        - & - & 1 & - & - \\
        1 & - & - & 1 & - \\
        - & 1 & - & 1 & - \\
        1 & - & - & 1 & 1 \\
      \end{bmatrix}
        & \times
          &
          \begin{bmatrix}
            \alpha_1 \\
            \alpha_2 \\
            \alpha_3 \\
            \beta_2 \\
            \beta_3 \\
          \end{bmatrix}
\end{array}
(\#eq:odp-sys-eq-matrix)
\end{equation}

```{remark}
$\mathbf{X}$ is the design matrix that defines the parameters used to estimate the losses in each cell
```

Use iteratively weighted least squares or MLE[^1] to solve for the parameters in the in $\mathbf{A}$ that minimize the squared difference between $\mathbf{Y}$ and $\mathbf{S}$, the solution matrix

[^1]: You can also use other methods such as orthogonal decomposition or Newton-Raphson to solve for the parameters

\begin{equation}
\mathbf{S} = \begin{bmatrix}
    ln[m_{1,1}] \\
    ln[m_{2,1}] \\
    ln[m_{3,1}] \\
    ln[m_{2,1}] \\
    ln[m_{2,2}] \\
    ln[m_{1,3}] \\
\end{bmatrix}
(\#eq:odp-sys-sol-matrix)
\end{equation}

After solving the system of equations we will have:

\begin{equation}
\begin{split}
  \ln[m_{1,1}] &= \eta_{1,1} &= \alpha_1 \\
  \ln[m_{2,1}] &= \eta_{2,1} &= \alpha_2 \\
  \ln[m_{3,1}] &= \eta_{3,1} &= \alpha_3 \\
  \ln[m_{1,2}] &= \eta_{1,2} &= \alpha_1 + \beta_2\\
  \ln[m_{2,2}] &= \eta_{2,2} &= \alpha_2 + \beta_2\\
  \ln[m_{1,3}] &= \eta_{1,3} &= \alpha_1 + \beta_2 + \beta_3\\
\end{split}
(\#eq:odp-sys-eq-solved)
\end{equation}

The above solution shown as a triangle below

Table: (\#tab:odp-glm-3) $3\times 3$ GLM fitted log incremental triangle:

|   w/d     |     1    |      2   |     3    |
|:------:|:--------:|:--------:|:--------:|
| **1**  | $\ln[m_{1,1}]$ | $\ln[m_{1,2}]$ | $\ln[m_{1,3}]$ |  
| **2**  | $\ln[m_{2,1}]$ | $\ln[m_{2,2}]$ |          |
| **3**  | $\ln[m_{3,1}]$ |          |          |

Exponentiate the triangle above to get our fitted (or expected) incremental results of the GLM model

Table: (\#tab:odp-glm-4) $3\times 3$ GLM fitted incremental triangle:

|   w/d     |     1    |      2   |     3    |
|:------:|:--------:|:--------:|:--------:|
| **1**  | $m_{1,1}$ | $m_{1,2}$ | $m_{1,3}$ |  
| **2**  | $m_{2,1}$ | $m_{2,2}$ |          |
| **3**  | $m_{3,1}$ |          |          |

#### Simplified GLM {#simplified-glm}

GLM model = Chainladder w/ volume-weighted averages when:

* Variance $\propto$ Mean

* $\varepsilon(w,d) \sim$ Poisson

* A parameter for each row and column (except 1^st^ column)

**Benefits**:

* Replace GLM fitting with much simpler calculation

* LDFs are easier to explain

* Still works even when there are negative incremental values

**Procedure** for fitting incremental triangle:

1. Select LDFs based on vol-wtd

2. Start from the last **cumulative diagonal** and divide backwards by each incremental LDFs to get the cumulative fitted triangle

3. Subtracting out the cumulative diagonals to get your incremental fitted triangle

### Residuals {#odp-res}

***Unscaled* Pearson residuals**

\begin{equation}
\begin{split}
  r_{w,d} & = & \dfrac{A - E}{\sqrt{\mathrm{Var}(E)}} &\\
  & = & \dfrac{q(w,d) - m_{wd}}{\sqrt{m^z_{wd}}} &\\
  & = & \dfrac{q(w,d) - m_{wd}}{\sqrt{m_{wd}}} & \:\:\:\: \text{Recall }z = 1\text{ for ODP Poisson}\\
\end{split}
(\#eq:odp-pear-res)
\end{equation}

* [Mean and variance](#odp-glm-para) as defined above

* Residual for the right and bottom corners of the triangle are going to be 0

    Because a unique parameter is used for those 2 cells
    
* Alternatively we can use *Anscombe* residual

    We prefer Pearson because its calculation is consistent with the scale parameter $\phi$

***Scaled* Pearson residuals** (England & Verrall)

\begin{equation}
  r^S_{w,d} = r_{w,d} \times \underbrace{\sqrt{\dfrac{N}{N-p}}}_{f^{DoF}}
  (\#eq:odp-scaled-res)
\end{equation}

* Degrees of freedom adjustment, to effectively allow for over dispersion of the residuals in the sampling process and add process variance to approximate a distribution of possible outcomes

* Increase the variability of the pseudo triangle

***Standardized* residuals** (Pinheiro et al.)

\begin{equation}
  r^H_{w,d} = r_{w,d} \times \underbrace{\sqrt{\dfrac{1}{1-H_{i,i}}}}_{f^H_{w,d}}
  (\#eq:odp-std-res)
\end{equation}

\begin{equation}
  \mathbf{H} = \mathbf{X}(\mathbf{X}^T\mathbf{WX})^{-1}\mathbf{X}^T\mathbf{W}
  (\#eq:odp-hat-matrix)
\end{equation}

\begin{equation}
\mathbf{W} = 
  \begin{bmatrix}
  m_{1,1} & 0 & \cdots & 0 \\
  0 & m_{2,1} & 0 & 0 \\
  \vdots & 0 & \ddots & \vdots\\
  0 & 0 & \cdots & m_{1,n}\\
  \end{bmatrix}
  (\#eq:weight-matrix-odp)
\end{equation}

* Hat matrix adjustment factor $f^H_{w,d}$ is based on the diagonal on the hat matrix $\mathbf{H}$

    (Going down the column of the triangle from left to right)
    
* $\mathbf{W}$ is a $2n \times 2n$ matrix

* $\mathbf{X}$ is the design matrix from \@ref(eq:odp-sys-eq-matrix)

* **Benefits**:

    1. $f^H_{w,d}$ account for the exclusion of zero-value residuals
    
        * Or the zero-value residuals will have some variance but we just don't know what it is yet so we should sample from the remaining residuals but not the zeros
        
    2. $f^H_{w,d}$ is an improvement on $f^{DoF}$

### Dispersion Factor {#odp-dispersion}

Dispersion factor

\begin{equation}
  \phi = \dfrac{\sum r_{wd}^2}{N-p}
  (\#eq:std-dispersion)
\end{equation}

$$N = \dfrac{n (n+1)}{2}$$

$$p = 2n-1$$

* $N =$ # of data points (**including** first column unlike [Ventor](#venter-goodness-fit))

    * $N$ can be less than indicated above if the tail incremental developments are all 0's

* $p =$ # of parameters

    * One for each row, one for each column minus first column
    
    * $p$ can be less than $2n-1$ if the later incremental values are all 0's and therefore not needed for fitting
    
* This calculation is similar to Clark's $\sigma^2$ \@ref(eq:clark-sigma-est)

Alternate method for $\phi$

$$\phi \sim \phi^H = \dfrac{\sum (r^H_{w,d})^2}{N}$$

* We can still use the same dispersion factor even with the *scaled* and *standardized* residuals, this just give us another method to estimate $\phi$

## Bootstrap Simulation Procedure {#odp-boot-sim}

Bootstrap simulation procedure, repeat steps 1 - 5 at least 10,000 times

0) Model our losses, determine [mean](#fitted-tri-odp) and [residual](#odp-res) for each cell

    * This can be based on [GLM Framework](#ODP-GLM-fit) of [simplified GLM](#simplified-glm)

1) Create a sampled $triangle^*$ from the residuals and the means

    i. **Sample** with replacement on the Pearson residuals \@ref(eq:odp-pear-res) from our original triangle from Step 0.
    
        (Since data needs to be $iid$ for bootstrap)
        
        (Note the distribution of the residual will be purely empirical)
    
    ii. **Simulated loss**:
        \begin{equation}
        q^*(w,d) = m_{wd} + r_p^* \sqrt{m_{wd}^z}
        (\#eq:odp-sim-loss)
        \end{equation}
    
        ($r^*_p$ is the realized sample from i.)
    
    iii. **Estimate [dispersion factor](#odp-dispersion)** $\phi$ for Step 3 as this is based on the original triangle
        
2) **Determine parameters** from $triangle^*$:

    * For [GLM Framework](#ODP-GLM-fit), calculate the $\alpha_w$'s, $\beta_d$'s
    
    * For [Simplified GLM](#simplified-glm) calculate the weighted average LDFs and Ultimate Loss

3) Calculate **mean** and **variance**[^2] for the future cells (unpaid): $(m^*_{wd}, \phi m^*_{wd})$

    * **Mean**: $m^*_{wd}$

        * *GLM Framework*:
        
            $m^*_{wd} = \mathrm{exp} \left [\alpha_w + \sum \limits_{i=2}^d \beta_i \right]$
    
        * *Simplified GLM*
        
            Back out the $c^*(w,d)$ by $\dfrac{Ult_w}{CDF_d}$ then get the $m^*_{wd}$ (Need to back out from ultimate because we need to complete the cells for unpaid too)
    
    * **Variance**: $\phi m_{wd}^{*}$
    
        (for ODP $z=1$ on the $m^{*z}_{wd}$)

[^2]: Step 3 and 3 were added in England & Verrall (2002), in their 1999 paper it doesn't have this step (stopped at 2) and instead just suggest you to add process variance by multiply the results by $f^{RoF}$

4) Add **process variance**: draw losses[^3] from the following Gamma distribution for the future cells (unpaid):

    \begin{equation}
      Gamma(m_{wd}^*,\phi m_{wd}^*)
      (\#eq:proc-var-sim-shapland)
    \end{equation}

    * Simulate loss from the gamma distribution for each future cells
    
    * Use $u \sim U[0,1]$ and $F^{-1}_{gamma}(u)$
    
[^3]: Poisson distribution can be used to remain more consistent with the underlying theory of GLM framework, but it is considerably slower to simulate, so gamma is a close substitute that performs much faster in simulation although it can be more skewed than the Poisson. Indeed other distributions could be used as well to better approximate the observed "skewness" of the residuals from the diagnostics

5) Calculate simulated **unpaid**: sum the bottom half of triangle
    
## Variations on the ODP Bootstrap {#bootstrap-variation-shap}

**Reason to use paid data**:

* For insurance risk it is best to focus on the **claim payment stream**:

    It measures the variability of the actual cash flows that directly affect the bottom line

* **Case reserves temper the volatility**
      
    Changes in case reserves and IBNR reserves will also impact the bottom line, but to a considerable extent the changes in IBNR are intended to counter the impact of the changes in case reserves
    
    To some degree, then, the total reserve movements can act to mask the underlying changes due to cash flows

**Reason to include case reserves**:

* Case reserves contain valuable information about potential future payments

### Bootstrapping the Incurred Loss Triangle

2 approaches to model the *unpaid loss* distribution using incurred loss triangle

**Method 1**: Modeling the incurred data and convert the ultimate values to a payment pattern

i. Run the paid and incurred data model in parallel
    
ii. For each iteration and each AY individually:

    Use the payment pattern (from paid model) to convert the ultimate values (from incurred model) to a payment stream

*Method 1* **Advantages**:

* We improve the ultimate estimates by incorporating the case reserves while still focusing on the payment stream for measuring risk

* Which effectively allows a distribution of IBNR to become a distribution of IBNR and case reserves
    
* Can make it more sophisticated by correlating some part of the paid and incurred models (e.g. the residual sampling and/or process variance portions)

    So that if we have large payment @ an older age, the incurred should be large as well
    
**Method 2**: Applying the ODP bootstrap to the Munich chain ladder model

* See Liu and Verrall (2010)

*Method 2* **Advantages**:

* Don't have to model the paid loss twice

* Explicitly measuring and imposing a framework around the correlation of the paid and outstanding losses

### Bootstrapping the BF and Cape Cod Method

**ODP issue**: Distribution for the most recent AYs can produce results with more variance than you would expect when compared to earlier AYs in the actual data

* Due More LDFs are used to extrapolate the sampled values for the most recent accident years and the random samples of incremental values

* Similar to the leverage effect of the deterministic chainladder

**Solution**: Incorporate BF or Cape Cod

* Have the a-priori be stochastic e.g. draw the BF a-priori from a distribution or apply Cape Cod to each simulated triangle

* More complicated approach is to modify the underlying assumptions of the GLM framework which would results in a completely different set of residuals (this is beyond scope)

## GLM Bootstrap Model {#GLM-bootstrap}

**Limitations** of *ODP Bootstrap* carry over from chainladder

1. Does not adjust for [CY trend](#GLM-variation-4) 

2. May over fit the data from using [too many parameters](#GLM-variation-3)

We can solve the above by going back to the [GLM framework](#ODP-GLM-fit) instead of using the [Simplified GLM](#simplified-glm) when we're at Step 2 of the [simulation](#odp-boot-sim)

**GLM benefits**

1. Not forced to use a specific number of parameters (e.g. GLM Variation [1](#GLM-variation-1) and [2](#GLM-variation-2))

2. Allows for [CY trend](#GLM-variation-4)

3. Can work with shapes that are non triangles (e.g. data with only the last $x$ diagonals)
    
    * We can forecast past the end of the triangle (e.g. have the $\beta$'s continue the decay)
    
    * Also see section on [practical issues](#prac-issue-4)
    
**GLM drawbacks**

1. Solving GLM at each iteration can slow down the process

2. Model not explainable using development factors

```{block, type='rmdtip'}
Below subsections are just examples of the variations discussed above
```

### GLM Variation 1: Reduce Row Parameters {#GLM-variation-1}

Use only 1 AY parameter $\alpha_1$

* Similar to Venter Cap Cod method \@ref(tab:venter-alt-pattern) with $h(w) = h$

* Moves away from the Chainladder assumption that each AY has its own level

* Also note that the residual in cell (3,1) will no longer be zero since it shares the $\alpha_1$ with all the other rows

* Below is our $3 \times 3$ triangle example analogous to our example in \@ref(eq:odp-sys-eq-matrix)

\begin{equation*}
\begin{array}{ccccc}
  \mathbf{Y} & = & \mathbf{X} &\times & \mathbf{A} \\
  & & \alpha_1 \:\:\: \beta_2 \:\:\: \beta_3 & &\\
  \begin{bmatrix}
    ln[q(1,1)] \\
    ln[q(2,1)] \\
    ln[q(3,1)] \\
    ln[q(1,2)] \\
    ln[q(2,2)] \\
    ln[q(1,3)] \\
  \end{bmatrix}
    & =
      &
      \begin{bmatrix}
        1 & - & - \\
        1 & - & - \\
        1 & - & - \\
        1 & 1 & - \\
        1 & 1 & - \\
        1 & 1 & 1 \\
      \end{bmatrix}
        & \times
          &
          \begin{bmatrix}
            \alpha_1 \\
            \beta_2 \\
            \beta_3 \\
          \end{bmatrix}
\end{array}
\end{equation*}

### GLM Variation 2: Reduce Column Parameters {#GLM-variation-2}

Use only 1 development year parameter $\beta_2$

* This just assumes the losses decay by $e^{\beta_2}$ for all ages

* Also note that the residual in cell (1,3) will no longer be zero since it shares the $\beta_2$ with all the other columns

* Below is our $3 \times 3$ triangle example analogous to our example in \@ref(eq:odp-sys-eq-matrix)

\begin{equation*}
\begin{array}{ccccc}
  \mathbf{Y} & = & \mathbf{X} &\times & \mathbf{A} \\
  & & \alpha_1 \:\:\: \alpha_2 \:\:\: \alpha_3 \:\:\: \beta_2  & &\\
  \begin{bmatrix}
    ln[q(1,1)] \\
    ln[q(2,1)] \\
    ln[q(3,1)] \\
    ln[q(1,2)] \\
    ln[q(2,2)] \\
    ln[q(1,3)] \\
  \end{bmatrix}
    & =
      &
      \begin{bmatrix}
        1 & - & - & - \\
        - & 1 & - & - \\
        - & - & 1 & - \\
        1 & - & - & 1 \\
        - & 1 & - & 1 \\
        1 & - & - & 2 \\
      \end{bmatrix}
        & \times
          &
          \begin{bmatrix}
            \alpha_1 \\
            \alpha_2 \\
            \alpha_3 \\
            \beta_2 \\
          \end{bmatrix}
\end{array}
\end{equation*}

### GLM Variation 3: Reduce Row and Column Parameters {#GLM-variation-3}

Further reduce the parameters to just 1 row and 1 column parameter $\alpha_1$ and $\beta_2$

* Flexibility of the GLM Bootstrap so that we're not always stuck with $p = 2n-1$ as stated [earlier](#odp-dispersion)

* This will gives us 6 residuals to sample from (the corners will not longer be 0's)

* Below is our $3 \times 3$ triangle example analogous to our example in \@ref(eq:odp-sys-eq-matrix)

* See [diagnostics](#odp-diagnostics) section on how to determine which parameters are statistically significant

\begin{equation*}
\begin{array}{ccccc}
  \mathbf{Y} & = & \mathbf{X} &\times & \mathbf{A} \\
  & & \alpha_1 \:\:\: \beta_2  & &\\
  \begin{bmatrix}
    ln[q(1,1)] \\
    ln[q(2,1)] \\
    ln[q(3,1)] \\
    ln[q(1,2)] \\
    ln[q(2,2)] \\
    ln[q(1,3)] \\
  \end{bmatrix}
    & =
      &
      \begin{bmatrix}
        1 & - \\
        1 & - \\
        1 & - \\
        1 & 1 \\
        1 & 1 \\
        1 & 2 \\
      \end{bmatrix}
        & \times
          &
          \begin{bmatrix}
            \alpha_1 \\
            \beta_2 \\
          \end{bmatrix}
\end{array}
\end{equation*}

### GLM Variation 4: Calendar Year Parameter {#GLM-variation-4}

We can also add calendar year trend $\gamma_k$ where $k$ is the CY

* $\gamma_2$ is the 2^nd^ diagonal and etc

* $\gamma_k$'s are **incremental** decay similar to the $\beta_d$'s

    $\therefore$ The total impact on the 3^rd^ diagonal is $e^{\gamma_2 + \gamma_3}$

* Note that the model here have 7 parameters and 6 values

    $\therefore$ It has no unique solution

* Below is our $3 \times 3$ triangle example analogous to our example in \@ref(eq:odp-sys-eq-matrix)

\begin{equation*}
\begin{array}{ccccc}
  \mathbf{Y} & = & \mathbf{X} &\times & \mathbf{A} \\
  & & \alpha_1 \:\:\: \alpha_2 \:\:\: \alpha_3 \:\:\: \beta_2 \:\:\: \beta_3 \:\:\: \gamma_2 \:\:\: \gamma_3 & &\\
  \begin{bmatrix}
    ln[q(1,1)] \\
    ln[q(2,1)] \\
    ln[q(3,1)] \\
    ln[q(1,2)] \\
    ln[q(2,2)] \\
    ln[q(1,3)] \\
  \end{bmatrix}
    & =
      &
      \begin{bmatrix}
        1 & - & - & - & - & - & -\\
        - & 1 & - & - & - & 1 & -\\
        - & - & 1 & - & - & 1 & 1\\
        1 & - & - & 1 & - & 1 & -\\
        - & 1 & - & 1 & - & 1 & 1\\
        1 & - & - & 1 & 1 & 1 & 1\\
      \end{bmatrix}
        & \times
          &
          \begin{bmatrix}
            \alpha_1 \\
            \alpha_2 \\
            \alpha_3 \\
            \beta_2 \\
            \beta_3 \\
            \gamma_2 \\
            \gamma_3 \\
          \end{bmatrix}
\end{array}
\end{equation*}

### GLM Variation 5: One Parameter for Each Dimension {#GLM-variation-5}

Again we can simplify things by having only 1 parameters for each dimension: $\alpha_1$, $\beta_2$, and $\gamma_2$

* Use this as a starting point then add or remove parameters as needed

* Below is our $3 \times 3$ triangle example analogous to our example in \@ref(eq:odp-sys-eq-matrix)

\begin{equation*}
\begin{array}{ccccc}
  \mathbf{Y} & = & \mathbf{X} &\times & \mathbf{A} \\
  & & \alpha_1 \:\:\: \beta_2 \:\:\: \gamma_2  & &\\
  \begin{bmatrix}
    ln[q(1,1)] \\
    ln[q(2,1)] \\
    ln[q(3,1)] \\
    ln[q(1,2)] \\
    ln[q(2,2)] \\
    ln[q(1,3)] \\
  \end{bmatrix}
    & =
      &
      \begin{bmatrix}
        1 & - & - \\
        1 & - & 1 \\
        1 & - & 2 \\
        1 & 1 & 1 \\
        1 & 1 & 2 \\
        1 & 2 & 2 \\
      \end{bmatrix}
        & \times
          &
          \begin{bmatrix}
            \alpha_1 \\
            \beta_2 \\
            \gamma_2 \\
          \end{bmatrix}
\end{array}
\end{equation*}

## ODP vs GLM Bootstrap Summary {#odp-vs-glm-shap}

**ODP Bootstrap** is a specific case of the GLM model with the following parameters:

* Parameters for every AY

* Parameters for every development year minus the first

* Variance of the incremental losses $\propto$ mean

Solution is the same as volume weighted Chainladder

**GLM Bootstrap** is the general model

* Can have as few as 1 row and/or column parameters

* Can include CY trend

* Variance of the incremental losses $\propto$ $m^z_{wd}$ (where $z=1$ for this paper)

## Practical Issues {#odp-prac-issues}

Practical issues we might run into with ODP bootstrap

1. [Negative Incremental Values](#prac-issue-1)

2. [Non‐Zero Sum of Residuals](#prac-issue-2)

3. [Using an L-year Weighted Average](#prac-issue-3)

4. [Missing Values](#prac-issue-4)

5. [Outliers](#prac-issue-5)

6. [Heteroscedasticity](#prac-issue-6)

7. [Heteroecthesious Data](#prac-issue-7)

8. [Exposure Adjustment](#prac-issue-8)

9. [Tail Factors](#prac-issue-9)

10. [Fitting a Distribution to ODP Bootstrap Residuals](#prac-issue-10)

### Negative Incremental Values {#prac-issue-1}

GLM doesn't work with negative incremental values because of $\ln[q(w,d)]$

Need to work around this in:

1. [Model fitting](#prac-issues-1.1) (e.g. Step 0 and 1 of the [Bootstrap process](#odp-boot-sim))

2. [Simulating](#prac-issues-1.2) for process variance with negative means (e.g. Step 4 of the [Bootstrap process](#odp-boot-sim))

Also [additional work around](#prac-issues-1.3) on extreme outcomes from negative values

#### Model Fitting {#prac-issues-1.1}

**Method 1**: Use $-ln(abs\{q(w,d)\})$

\begin{equation}
Cell_{w,d} = 
\begin{cases}
  \ln[q(w,d)] & \text{if } q(w,d) > 0 \\
  0 & \text{if } q(w,d) = 0 \\
  -\ln[abs \{ q(w,d) \}] & \text{if } q(w,d) < 0 \\
\end{cases}
(\#eq:method-1-inc-neg-fit)
\end{equation}

```{remark}
Doesn't work when the column sum to a negative value

* This is done when setting the design matrix \@ref(eq:odp-sys-eq-matrix)
```

**Method 2**: *Subtract* a *negative* constant $\Psi$

\begin{equation}
q^+(w,d) = q(w,d) - \Psi \\
\ln[q^+(w,d)] \text{   for all   } Cell_{w,d}
(\#eq:method-2-inc-neg-fit)
\end{equation}

* Pick $\Psi =$ largest negative value in the column

* Apply \@ref(eq:method-2-inc-neg-fit) before solving the GLM system of equations (e.g. \@ref(eq:odp-log-mean) and \@ref(eq:odp-sys-eq))

* Then adjust the fitted values by adding back $\Phi$ to reduce each fitted incremental value

\begin{equation}
m_{w,d} = m^+_{w,d} + \Psi
(\#eq:method-2-inc-neg-fit-adj)
\end{equation}

* Can use this method combined with method 1 to take care of the extra large negative ones

* Need to make use the absolute value for the residual and re-sampling formula, modify \@ref(eq:odp-pear-res) and \@ref(eq:odp-sim-loss) with below:

\begin{equation}
  r_{wd} = \dfrac{q(w,d)-m_{wd}}{\sqrt{abs\{m^z_{wd}\}}}
  (\#eq:odp-pear-res-adj-neg)
\end{equation}

\begin{equation}
  q^*(w,d) = m_{wd} + r^*_p \sqrt{abs\{m^z_{wd}\}}
  (\#eq:odp-sim-loss-adj-neg)
\end{equation}

**Method 3**: Use [simplified GLM](#simplified-glm)

* Use ODP bootstrap (i.e. Chainladder with volume weighted average LDFs)

* This will yield different estimate than using the GLM framework with adjustment 1 or 2
    
#### Simulating Negative Values {#prac-issues-1.2}

From [above](#prac-issues-1.1), we might have the fitted $m_{wd}$ that are negative, which will be an issue when used in Step 4 of the [bootstrap simulation](#odp-boot-sim), when we need to model the process variance with $Gamma(m_{wd},\phi m_{wd})$

* Since $Gamma$ only takes positive parameters

**Adjustment to the *Gamma* Distribution** with negative $m_{wd}$

\begin{equation}
  Gamma(abs\{m_{wd}\}, \phi abs\{m_{wd}\}) + 2m_{wd}
  (\#eq:gamma-adj-odp)
\end{equation}

* This will maintain the right skew of *Gamma* while having the mean of $m_{wd}$

* Alternatively if we use $-Gamma(abs\{m_{wd}\}, \phi abs\{m_{wd}\})$ it'll flip the curve to skew left

#### Extreme Outcomes from Negative Values {#prac-issues-1.3}

Column with negative mean in the early ages can results in *vary large LDFs* (and lead to simulated outcomes that are 1,000 times greater than our mean)

* Negative mean causes one column of cumulative values to sum close to 0 and the next to sum to a much larger number resulting in extremely large LDF and there for projection that are extremely large

* Need to address this as it'll throw off the mean even if you don't care about the high percentiles

3 options to address this:

1) **Remove the extreme iterations**

    Beware of understating the the likelihood of extreme outcomes

2) **Recalibrate** the Model

    * First need to identify the source of the negative losses

    * Review data used and parameter selection
    
        * e.g. remove the AYs that might not represent current behavior
        
        * e.g. if due to S&S then you can just model them separately and then correlate them during simulation

3) **Limit Incremental Losses to 0**

    Either with the simulated mean (Step 2) or the process var step (Step 4)
    
    * Replace with negatives with 0s

    * Can just do it in certain columns

### Non-Zero Sum of Residuals {#prac-issue-2}

Residuals are supposed to be *iid* with mean zero and constant variance

$\therefore$ Sum of our residuals from the triangle should be 0

* Not necessarily the case since this is just a sample

**Consequence**: Simulated outcomes will be higher than the mean if sum of residuals are positive (and vice versa)

**2 options** to address this:

1. Keep it if we believe this to be characteristics of the data set

2. Add a constant to each non-zero residual so that it sums to 0

    Then sample from the adjusted residuals
    
If residuals are significantly different from zero then the fit of the model should be questioned

### Using L-year Weighted Average {#prac-issue-3}

Select LDFs based on the latest $L$ years

**GLM Bootstrap**

* Only use $L+1$ diagonals of data to get $L$ diagonals of LDFs

* Excluded diagonals are given zero weight and we'll have less CY trend parameter (if we're using it)

* In the simulation we'll only sample residuals for the trapezoid used to parameterize the model
    
    (since that's all we'll need to estimate parameters)

**Simplified GLM**

1. Get *L*-year weighted average LDFs

2. Will only have residuals (to sample from) for the most recent *L* + 1 diagonals

3. In the simulation we'll create the **entire resampled triangle**

    (Since we need the cumulative losses for each row)
    
4. For projection using the resampled triangle we'll still only use the *L*-year average LDFs

The 2 methods will results in different results

* GLM Bootstrap: Models the incremental losses in the trapezoid

* Simplified GLM: Models the same losses but in relation to the cumulative losses, which include the non-modeled losses in the diagonals excluded

### Missing Value {#prac-issue-4}

**ODP Bootstrap**:

Missing data impact:

* LDFs

* Fitted triangle (if missing value lies on the most recent diagonal)

* Residuals

* Degree of freedom

**Solutions**:

* Impute from surrounding values

* Modify LDFs to exclude missing value

Similar to the *L*-year weighted average:

* Missing value will be resampled so the cumulative losses can be calculated

* Projection from the resampled triangle will exclude the missing cell for resampled LDF selection

**GLM Bootstrap**

Impact on the is limited, we'll just have less observations

### Outliers {#prac-issue-5}

Remove outliers if they are not representative of the variability of the losses, below are the options:

* Remove the entire row (easy if it's the 1st row of the triangle)

* Remove the values and treat them as missing values

* Not use the residual but do create a sampled value in that cell

Significant number of outliers might indicate **bad model fit**

**GLM Bootstrap**

* Pick new parameters (grouping parameters)

* Change the error term distribution from $z=1$

**ODP Bootstrap**

* Use *L*-year weighted average

* Heteroscedasticity may exist

    * See adjustment [next sub section](#prac-issue-6) and [diagnostics](#odp-res-diag)

* Since we dont' make a distribution assumption, the number of outliers could mean the data is quite skewed and it's appropriate that is showing up in the simulation

### Heteroskedasticity {#prac-issue-6}

Issue of non constant variance

* ODP bootstrap assumes residuals are $iid$ with constant variance

* No longer possible to sample the residuals from the whole triangle with heteroskedasticity

**GLM Bootstrap** has the additional flexibility of choosing *parameters* to alleviate heteroscedasticity

For **ODP Bootstrap**: 3 ways to deal with heteroscedasticity below

* They also work for **GLM Bootstrap**

#### Stratified Sampling {#stratified-sample}

Stratified Sampling

1. Split the triangle into groups with similar variance

2. Only sample residuals from the same group

**Cons**

* Each group may not be that large, which limits the amount of variability in the possible outcomes

#### Hetero-Adjustment to the Residuals {#hetero-adj-res}

Calculate a hetero-adjustment factor to scale the residuals to the same level:

1. Group the residuals with similar then calculate the $\sigma$ of the residuals in each group $i$

2. Hetero-adjustment factor: $h^i$

    i.e. The largest $\sigma$ $\div$ each group's $\sigma$

\begin{equation}
  h_i = \dfrac{\sigma \left( \bigcup_1^j r^H_{wd} \right)}{\sigma \left( \bigcup_i r^H_{wd} \right)} \:\: : \:\: \text{for each } 1 \leq i \leq j
  (\#eq:odp-hetero-adj-factor)
\end{equation}

3. Scale up the residuals:

    Residual \@ref(eq:odp-pear-res) $\times$ Hat Matrix Factor \@ref(eq:odp-std-res) $\times$ Hetero Factor \@ref(eq:odp-hetero-adj-factor)

    \begin{equation}
      r_{wd}^{iH} = r_{wd} \times f_{wd}^H \times h^i
      (\#eq:odp-hetero-adj)
    \end{equation}
    
    * $h^i$ here is based on the group we draw from

4. Need to divide the sampled residual by $h^i$ to reflect the variability of group $i$

    \begin{equation}
      q^{i*}(w,d) = m_{wd} +       \dfrac{r^{i*}}{h^i}\sqrt{m_{wd}}
      (\#eq:odp-hetero-adj-rescale)
    \end{equation}

    * $h^i$ here is based on the group we're simulating for

5. Adjust the variance for the process variance step in the simulation

\begin{equation}
  Gamma(m_{wd}, \dfrac{\phi m_{wd}}{h^i})
  (\#eq:gamma-adj-odp-hetero)
\end{equation}

```{remark}
The hetero adjustment factors are new parameters and will impact degrees of freedom and will impact the scale parameter \@ref(eq:std-dispersion) and the degrees of freedom adjustment factor \@ref(eq:odp-scaled-res)
```

#### Non-constant Scale Parameters {#non-constant-scale-para}

Adjust the dispersion factor $\phi$ as well as the residuals (similar to above)

1. Calculate the hetero adjustment factor $h_i$ using formula \@ref(eq:non-cont-scale-phi) below:

\begin{equation}
  h_i = \sqrt{\dfrac{\phi}{\phi_i}}
  (\#eq:odp-hetero-adj-factor-2)
\end{equation}

2. Perform step 3 and 4 from the [hetero adjustment method](#hetero-adj-res) above

    (i.e. Equation \@ref(eq:odp-hetero-adj) and \@ref(eq:odp-hetero-adj-rescale))

3. Calculate $\phi_i$ for each homogenious residual group $i$ ($n_i$ = number of residuals in group):

\begin{equation}
  \phi_i = \dfrac{N}{N-p}\dfrac{\sum_{w,d \in \{i\}}r^2_{w,d}}{n_i}
  (\#eq:non-cont-scale-phi)
\end{equation}

4. Use $\phi_i$ for the process variance step

```{remark}


* The $\phi_i$ here also amount to new parameters that will impact the degrees of freedom adjustment factor \@ref(eq:odp-scaled-res)

* The hetero adjustment factor \@ref(eq:odp-hetero-adj-factor-2) is more theoretically sound but in practice very similar to \@ref(eq:odp-hetero-adj-factor)

```

### Heteroecthesious Data {#prac-issue-7}

ODP bootstrap requirements:

1. Symmetrical shape (annual by annual, quarter by quarterly, etc triangles)

2. Homoecthesious data (similar exposure)

**Heteroecthesious** = Accident years have different level of exposures

Here we are focusing on heteroecthesious due to interim evaluation dates:

1. [Partial first development period](#prac-issue-7.1)

2. [Partial latest calendar period](#prac-issue-7.2)

#### Partial First Development Period {#prac-issue-7.1}

This means the entire first development period is shorter than the rest

* e.g. Annual data evaluated as of 6/30 with 1/1-12/31 AYs

    We'll have a triangle with development periods @6, 18, 30, 42, etc

Pearson residuals use the square root of the fitted value to make them all exposure independent (debatable...)

* $\therefore$ **No impact to residuals**

**Adjustment**: Scale down the most recent AY projection to the appropriate exposure period (e.g. half the exposure based on example above), we have *2 options*:

* Prorate the mean of the incremental cells for the latest AY between step 3 and 4 of the [bootstrap process](#odp-boot-sim) and then proceed to Step 4 for the process variance as usual

* Prorate the simulated incremental cells for the latest AY *after* the process variance step (Step 4)

#### Partial Latest Calendar Period {#prac-issue-7.2}

This is where the latest diagonal is partial diagonal

* e.g. Evaluate in between typical data evaluation date

    Evaluation @6/30 for a 1/1-12/31 AYs and 12-24-36 triangle
    
* Similar problem as partial first development period + partial data in most recent diagonal

**ODP Bootstrap**

Select LDF by excluding latest diagonal or prorating the latest diagonal to full year

Adjusted [simulation process](#odp-boot-sim)

1. Calculate sampled triangle as usual (diagonal will be of full year)

2. Calculate full year LDFs and Ultimate as usual

3. *Additional steps*:

  * De-annualize the diagonal
  
  * Interpolate the full year LDFs to match the diagonal
  
  * Forecast loss
  
  * Scale down the latest AY similar to the [partial AY adjustment](#prac-issue-7.1)
  
4. No change

**GLM Bootstrap**

* Should be something similar

### Exposure Adjustment {#prac-issue-8}

Adjustment for when exposure changed dramatically over the years (e.g. rapid growth or run off)

**ODP Bootstrap**

* Divide losses by exposure (model loss cost)

* Need to multiply the simulated results by the exposure (after the process variance step)

**GLM Bootstrap**

* Adjust losses by exposure similar to above

* Fit to the exposure adjusted losses should be exposure weighted

    (*i.e. exposure adjusted losses with higher exposure are assumed to have lower variance, see Anderson et al. (2007)*)

* This will need fewer AY parameters since the exposure adjustment should capture a lot of the difference between AYs

### Parametric Bootstrapping {#prac-issue-9}

**ODP Bootstrap**

* See CAS Tail Factor Working Party Report (2013)

* Add tail factor to the algorithm by assuming the factor follows a distribution (other considerations such as process variance, hetero-adj can all be extended to include the tail factors)

* Should be an extrapolation of the incremental tail factors (instead of a single tail factor to ultimate)

* Tail factors typically have $\sigma <$ 50% of the tail factor - 1

    (But should compare to the $\sigma$ of the AtA factors leading up to the tail in both the actual and simulated data)

**GLM Bootstrap**

* Continue to use the last $\beta_d$ to estimate the tail by continuing to apply it (similarly for CY parameter)

### Fitting a Distribution to ODP Bootstrap Residuals {#prac-issue-10}

Data points from triangle may not be representative of the underlying distribution

* Whether the most extreme observation is a 1-in-100, 1-in-1000 event

Alternative is to fit a distribution to the residuals and sample from the distribution instead i.e. parametric bootstrapping

## Diagnostics {#odp-diagnostics}

Use diagnostics to judge the quality of the model:

1) Test model assumptions

2) Gauge quality of model fit

3) Guide the adjustments of model parameters

5 diagnostics

1. [Residual graphs](#odp-res-diag)

2. [Normality test](#odp-norm-test)

3. [Outliers](#odp-diag-outlier)

4. [Parameter adjustment](#odp-diag-para)

5. [Model results](#odp-diag-results-review)

### Residual Graphs {#odp-res-diag}

Plot **residuals** versus

* CY, AY, Age, forecast loss (on x-axis)

* Want to see random variability around zero

* Bare in mind that we don't have the same number of residuals at each point (helpful to plot the line for average as well)

Test assumption of *iid* residuals across the entire triangle

This helps with grouping for [hetero adjustment](#prac-issue-6)

* Plot the relative $\sigma(r_{wd})$ for each group to further help with the groupings

* Do all the plots again after adjustment

### Normality Test {#odp-norm-test}

Normality is not required, only need this if we're doing parametric bootstrap with normal distribution

Plot residuals against the normal best fit based on the percentiles

* QQ-plot

Statistical tests:

* Check if p-value > 5%

* $R^2$

* AIC

$$2p + n \left [ 1 + \ln(2\pi\dfrac{RSS}{n})\right]$$

* BIC

$$n \ln\left( \dfrac{RSS}{n}\right) + p \ln(n)$$

* $RSS$ = actual residual - expected residual from normal then squared and summed

### Outlier {#odp-diag-outlier}

Remove true outliers *but* do not remove points that are realistic extreme scenarios

Use box & whisker plot

* Box hows 25%-tile to 75%-tile

* Whiskers are 3 times the inter quartile range (both side total)

* Residuals outside the range are graphed

### Parameter Adjustments {#odp-diag-para}

Test model with different sets of parameters using [GLM bootstrap](#GLM-bootstrap)

* Check parameter significance based on *t*-statistics (>2)

*Parameter selection process*:

1. Start with all the AY and Age parameters ($\alpha_w$ and $\beta_d$) and remove the insignificant ones until only significant parameters are left

2. Add CY parameter ($\gamma$) and check for significance

*After selecting parameters*:

3. Check the diagnostics discussed above (e.g. [residuals](#odp-res-diag) and [normality](#odp-norm-test))

4. Make hetero adjustment if necessary 

5. Compare implied development with ODP

### Review Model Results {#odp-diag-results-review}

Review outputs once we have decided on a model and run the bootstrap

* Mean, s.e., CoV, Min/Max, and percentiles by AYs

* Incremental fitted mean, s.e. and CoV for each cell in the triangle

    * Check for reasonability and consistency

Table: (\#tab:odp-output-table) Model output review format 

| AY | Mean Unpaid | Standard Error | CoV | Min | Max | 50%-tile | 75%-tile | 95%-tile | 99%-tile |
|:---:|:------:|:------:|:------------------:|:------:|:------:|:------:|:------:|:------:|:------:|
|  | (1) | (2) | (3) = (2) / (1) | (4) | (5) | (6) | (7) | (8) | (9) |
| 1 | - | - | - | - | - | - | - | - | - |
| $\vdots$ | - | - | - | - | - | - | - | - | - |
| $w$ | - | - | - | - | - | - | - | - | - |
| Total | $\sum$ | - | - | - | - | - | - | - | - |

```{remark}

***Standard Error***: Col (2)

* Total s.e. should be **greater than** any individual year but less than the straight sum of each AY's s.e.

* Expect s.e. to **increase** going down the column

* This is different from Mack? Where we expect the total s.e. to be greater than the simple sum due to correlation between AYs? (questionable statement here)
```

```{remark}

***Coefficient of Variation***: Col (3)

* Total CoV should be **less than** any individual year (due to diversification of results across AYs)

* Except for the most recent AYs, CoV should **decrease** going down the column (due to larger based of unpaid losses for the more recent AYs)
    
* Higher CoV for the most recent AYs due to:
    
    i. More parameters used to forcase for the most recent AYs $\therefore$ parameter uncertainty $\gg$ process variance
    
    ii. Model maybe overestimating the uncertainty $\Rightarrow$ Use *BF* of Cape Cod
```

```{remark}

***Min & Max*** Col (4) - (5)
    
* Check for reasonability (e.g. [extreme outcomes from negative values](#prac-issues-1.3))

* Implausible results can affect the mean
```

## Using Multiple Models {#shapland-multi-mod}

Use different methods (Paid/Inc'd Dev, BF, etc) and assigning weights by AYs

* Models should be reviewed and finalize individually before blending with weights

**Method 1**:  
In the process variance step of bootstrap, use the same underlying $u \sim U(0,1)$ to draw from each model then weight the models by a set of deterministic %'s

* Use the same random variable or else we would reduce the variability of the outcomes

**Method 2**:  
Run each model independently for each simulation (i.e. use different $u \sim U(0,1)$) then for each AY use the weights to randomly select one of the modeled results

* Results will be a mixture of the various models

**Other considerations**:

* Should consider both the mean and standard deviation (or CoV) in each model result when selecting weights

* Can also select the weights using Bayesian methods to account for the quality of each model's forecast

* Perform the same [model output review](#odp-diag-results-review) as in the above section for the best estimate

* Also review the IBNR by AYs to look for inconsistencies (e.g. negative IBNR) and compare to deterministic results

### Additional Useful Output

Using the best estimate total unpaid *mean*, *s.e.*, and *CoV* from above to fit to Normal, LogNormal, and Gamma distribution.

We can use these fitted distribution to:

1. Assess quality of fit

2. Parameterize a DFA model

3. Smooth out extreme values

### Estimated Cash Flow Results

Since bootstrap generates simulation for each cell in the bottom half of the triangle we can use this to get cash flow forecasts by CY and their variability as well

We can review the s.e. and CoV similar what we did in the [diagnostics](#odp-diag-results-review) section

### Estimated Ultimate Loss Ratio Results

We can estimate mean and the variability of ultimate loss ratios by AYs

Compile a similar table as before \@ref(tab:odp-output-table) but for loss ratio

Useful for projecting pricing risk in a risk model

### Estimated Unpaid Claims Runoff Results

Project unpaid claims out by CY similar to the cash flow projection

Useful for calculating risk margins using the cost of capital method

### Distribution Graphs

Plot the distribution of the simulated unpaid in a histogram

* Or smooth the histogram with a Kernel density function (for each point it takes a weighted average of the points around it, giving less weight to points further from it)

* For each point it takes a weighted average of the points around it; giving less weight to points further from it

### Correlation

Correlate the loss distribution over several LoB

* Multivariate distribution requires the same underlying distribution which doesn't work here for ODP

Method 1: **Location Mapping**

* When sampling the residuals, sample from the same place in the triangle for all the lines we want to correlate

* **Disadvantages**:

    * Requires all LoB to have the same size triangle with no missing values or outliers

    * Cannot stress the correlations among the LoBs (Can only use the historical correlations)

Method 2: **Re-Sorting**

* Use Iman-Conover algorithms or Copulas (Not explained in paper)

* **Advantages**:

    * Can accommodate different shapes and sizes

    * Can make different correlation assumptions

    * Can strengthen the correlation for extreme events (e.g. *t* Copula vs normal Copula)

* Calculate correlation matrix using Spearman's Rank Order and re-sorting based on the ranks of unpaid claims by AYs

    * Look at p-value for each correlation parameter to see that they're significantly different from zero

Additional comments:

* Using residuals to correlate LoBs (both location mapping& re-sorting) are liable to create correlations close to zero

* *Reserve Risk*: Correlate **total unpaid** by correlating the incremental paid
    
    May or may not be a reasonable approximation
    
    Risk not modeled is contagion risk, where a single event results in claims in multiple lines of business (can change correlation assumptions to address this)

* *Pricing Risk*: Correlate **loss ratios** over time

    Not as likely to be close to zero

    Use different correlation assumption than for reserve risk

## Model Testing {#shapland-testing}

Important to test the model results against actual (see [Meyers](#meyers-test))

**Challenges**:

* Don't have underlying distribution to compare with

* Can't wait for 10 years to see how it forecast

**General Insurance Reserving Oversight Committee test**:

* GIROC created triangles that met the assumptions of Mack and ODP

* 30K 10 $\times$ 10 triangles were created

* Mack: losses were above the 99^th^ percentile about 8-13% of the time $\Rightarrow$ Mack underestimates tail events

* ODP from England & Verrall: losses were above the 99^th^ percentile about 3% of the time

    * This does not include the residual adjustments
    
Future testing:

* Create datasets from claims transaction data and use them for model testing

### Future Research

* Test ODP bootstrap on realistic data from CAS loss simulation model

* See how adjustments discussed here improve predictive power

* Expand ODP bootstrap with

    * Munich Chainladder for incurred/paid
    
    * Claim counts and average severity
    
    * Different residuals (e.g. deviance or Anscombe residuals)

* Select weights using Bayesian methods by AYs

* Other risk analysis measures and use for ERM

* SII requirements

* Research in correlation matrix (difficult to estimate)

## Past Exam Questions

**TIA Exercise**

* $\star \star$ TIA 1: Use simplified GLM and then back out the GLM parameters

* TIA 2: Reduce parameters

* TIA 3: Minimize square error for GLM

* TIA 4: Benefit of simplified GLM

* $\star$ TIA 5: Residuals with england verrall adjustment

* TIA 6: Dispersion

* TIA 7: Dispersion with hat matrix adj

* TIA 8: Simulate loss variance distribution assumption & simulate psudeo triangle (no process variance)

* TIA 9: Setup GLM with CY trend

* TIA 10: Negative values

* TIA 11: Simulate negative

* TIA 12: Partial triangle

* TIA 13: Stratified (includes "new" method in the paper)

* TIA 14: Dealing with correlation (location mapping)

* TIA 15: Outliers

**Practical Issues**

* 2013 #7: negative values

* 2014 #7: List 4 practical issues and solutions

* 2015 #10: Heteroscedasticity, why important, adjustments description

* 2015 #11: Identify issues with given triangle: Negative values, outliers, exposure level

* 2016 #12: Negative values adjustment

**Diagnostics**

* $\star$ 2014 #9: Evaluate the results given mean unpaid, s.e., CoV by AYs

* $\star$ 2016 #13: Residual plots, part c on rational

**Calculations**

* $\star \star$ 2016 14: Hetero adjustment

### Question Highlights

n/a