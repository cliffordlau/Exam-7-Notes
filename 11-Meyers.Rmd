# Stochastic Loss Reserving Using Bayesian MCMC Models - G. Meyer

[3 reasons](#meyers-intro) model doesn't predict well

**Interpretation** of all the [test](#meyers-test):

* KS-test: \@ref(eq:ks-expected-lower), \@ref(eq:ks-test-meyers-diff), and \@ref(eq:ks-test-meyers-criteria)

* $p-p$ plot (fig. \@ref(fig:meyers-p-p-plot))

    * Too light tailed: Shallow slope near corner and steep in the middle

    * Too heavy tailed: Steep slope near corner and shallow in the middle

    * Biased upwards: Bow down

* Freq vs Count plot (fig. \@ref(fig:meyers-freq-perc-plot))

All the different [model](#model-summary-meyers) introduced (fig. \@ref(fig:meyers-models-overview))

***Bayesian Models*** (*Cumulative*):

* Lognormal \@ref(eq:meyers-loss-dist-lcl)

* $\beta = 0$ when it's done developing

* $\sigma$ constraint \@ref(eq:meyers-sigm-dist-lcl)

*Variations*

* [Leveled Chain-Ladder](#meyers-lcl) (LCL): Add variability to the row parameter with $\alpha$

    * Mean \@ref(eq:meyers-lcl-mean)

* [Correlated Chain-Ladder](#meyers-ccl) (CCL): Add AY correlation with $\rho$

    * Mean \@ref(eq:meyers-ccl-mean)

* [Changing Settlement Rate](#meyers-csr) (CSR): LCL with speed up claims closure with $\gamma$

    * Mean \@ref(eq:meyers-csr-mean)
    
    * $\gamma >0$ for increase payout speed

**Bayesian Models** (*Incremental*):

* Distribution \@ref(eq:meyers-CIT-distribution)

    * Based on mixed lognormal distribution \@ref(eq:meyers-mix-log-norm-norm) (skewed log-normal \@ref(eq:meyers-skewed-normal) was not used)

* $\sigma$ constraint \@ref(eq:meyers-sigm-dist-cit) is different

* Additional constraint on $\beta$ so that it is decreasing

* Constraint on CY trend $\tau$

* Additional constraint on $\sigma$ so that it cannot increase drastically period to period

*Variation*

* [Correlated Incremental Trend](#meyers-cit) (CIT): LIT with added AY correlation $\rho$

    * Mean \@ref(eq:meyers-CIT-mean)
    
* [Leveled Incremental Trend](#meyers-lit) (LIT): Use skewed distribution and CY trend $\tau$

**Non-Bayesian Models**:

* [Mack](#meyers-mack): See [Mack-1994](#cl-ass)

* [England & Verall ODP](#meyers-odp): See [Shapland](#odp-glm-para), but doesn't have the [residual adjustments](#odp-res)

## Introduction & Synopsis {#meyers-intro}

Applying different estimate methods to 200 triangles and comparing the actual outcomes to the predicted distribution to see if the models accurately estimates the distribution of outcomes

Models examined:

* Estimate from Mack and ODP do not have enough variability

* Incurred losses with variable row parameters and AY correlation $\rho$ is sufficient

* Paid losses requires variable row parameters and change in settlement rate parameter $\gamma$

**Three reasons model doesn't predict well**:

1) Insurance loss environment has experienced changes that are not observable at the valuation date

    i.e. There would be different "black swan" events that invalidate any attempt to model loss reserves

2) There could be other models that better fit the existing data

3) Data used to calibrate the model is missing crucial information needed to make a reliable prediction

    e.g. Changes in the way the underlying business is conducted, like changes in claim processes of changes in the direct/ceded/assumed reinsurance composition of the claims triangle

If we can find better model and/or better data we can rule out 1)

* If we review many models and none of them validate it gives 1) credence but does not confirm

## Data Set

200 triangles from Sch P 1997 and reviewed 10 years later

* Paid and incurred

* 50 triangles from 4 LoB (Commercial Auto, Personal Auto, WC, Other Liab)

**Potential pitfalls** in sample triangle selections

1) Insurer with significant changes to their books over the exposure period would violate the assumptions of the model and should be excluded

    * **Solution**: Use consisteny of **NEP** and **Net:Gross Premium** to establish stability in book
    
    * Use CoV to establish consistency
    
    * Pick triangles with CoV below a threshold

2) Need to avoid selecting datasets that best suit the model e.g. removing "outliers" from the data

    * **Solution**: Choose the company in an automated and well defined manner

Losses are considered fully developed at 10 years so in practice the paid and incurred ultimate is slightly different

## Testing Procedure {#meyers-test}

We are testing the **process** of each method and not the results of any one distribution generated from the method

* Since there's only 1 actual outcome for each triangle we test

* Focus on the process (the different methods) that gives us the distribution

* Compare the predicted percentiles (from the methods) against the expected percentile

* Typically we can compare distributions by comparing the density function, but we can not in this case because 1) We have a small data sets (50 points) 2) Each data point comes from a different distribution

**Testing Procedure**

* Given $N$ triangles and their actual outcome in 10 years

* Generate $N$ sets of distribution (from the $N$ triangles using one of the methods) and determine the predicted percentile $p_i$ based on the predicted distribution

    * i.e. see where the actual outcome lands on our predicted distribution

* The distribution of the $N$ predicted percentiles $p_i$ should follow a uniform distribution if the model is accurate, so we rank them to form $\{p_i\}$

$$\{p_i\} = \{p_1,...,p_n\}$$

* The expected percentiles $\{e_i\}$ should run from $\frac{1}{n+1}$ to $\frac{n}{n+1}$
    
$$\{e_i\} = \left\{ \dfrac{1}{n+1}, \dfrac{2}{n+1},...,\dfrac{n}{n+1} \right\}$$

### Kolmogorov-Smirnov Test {#meyers-ks-test}

For the *KS* test we'll compare $\{ p_i \}$ with $\{ f_i \}$

\begin{equation}
  \{f_i\} = \left\{ \dfrac{1}{n}, \dfrac{2}{n},...,\dfrac{n}{n} \right\}
  (\#eq:ks-expected-lower)
\end{equation}

$H_0$: Distribution of $p_i$ is uniform

**Test statistics** for maximum difference between the predicted and expected percentiles

\begin{equation}
  D = \max \limits_i \mid p_i - e_i \mid
  (\#eq:ks-test-meyers-diff)
\end{equation}

**Reject** $H_0$ @5% confidence level if:

\begin{equation}
  D > \dfrac{136}{\sqrt{n}}\%
  (\#eq:ks-test-meyers-criteria)
\end{equation}

* i.e. For $n = 50$ $\Rightarrow$ 19.2%; $n=200$ $\Rightarrow$ 9.6%

**Example**

Table: (\#tab:ks-test-example) Kolmogorov-Smirnov test example

| $f_i$ | $p_i$ | $abs\{ p_i - f_i \}$ |
|:---------:|:---------:|:------------------:|
| (1) | (2) | (3) |

1. Col (1): $\{f_i\} = \left\{ \dfrac{1}{n},...,\dfrac{n}{n}\right\}$

2. Col (2): $\{p_i\}$ = $p_i$ from each realization of the triangles sorted in ascending order

3. Col(3) = Absolute difference between the first two columns

4. $D$ is the maximum value from column (3)

5. Compare $D$ with $\dfrac{136}{\sqrt{N}}$

    If $D$ is less than the critical value we do not reject the $H_0$ that $\{ p_i \}$ is uniform

```{remark}
Technically based on Klugman you test against both:

$$\{f^+_i\} = \left\{ \dfrac{1}{n}, \dfrac{2}{n},...,\dfrac{n}{n} \right\}$$

and 

$$\{f^-_i\} = \left\{ \dfrac{0}{n}, \dfrac{2}{1},...,\dfrac{n-1}{n} \right\}$$
```

Alternatively, we can use *Anderson-Darling* test that focuses on the tail

* But it failed all the models therefore we do not use it as it does not help in model comparison

### *p-p* Plot

We plot the $p-p$ plot with $e_i$ vs $p_i$ to diagnosis

* Dark blueline is what is expected from uniform distribution

* Light blueline is the critical value for a given $n$ from the [*KS* test](#meyers-ks-test) above

```{r meyers-p-p-plot, echo = FALSE, out.width='100%', fig.show='hold', fig.cap='p-p plot'}
knitr::include_graphics('figures/Exam-7-Notes-10.png')
```

* Model is too light tailed: Shallow slope near corner and steep in the middle

* Model is too heavy tailed: Steep slope near corner and shallow in the middle

* Model is biased upwards: Bow down

    * Biased upwards: predicted mean > true mean while the s.d. is correct
    
* If we look at lognormal data fitted to normal, we'll see a mix of light and heavy tailed model

    * i.e. The right tail will be too light so we'll see a shallow slope in the right and the left tail will be too light so we'll see a steep slope at the left

### Percentile Histogram

We plot a (flipped) histogram

* y-axis being the predicted percentile

* x-axis is the frequency

* Bins with width 0.1 (so 10 bins)

* Vertical blue line represent the expected frequency based on uniform $\{e_i\}$

    * Expected frequency = $\dfrac{\text{# of points}}{\text{# of bins}}$= $\dfrac{n}{10}$

```{r meyers-freq-perc-plot, echo = FALSE, out.width='100%', fig.show='hold', fig.cap='Percentile Histogram'}
knitr::include_graphics('figures/Exam-7-Notes-12.png')
```

## Models Overview {#model-summary-meyers}

Starting point for the Bayesian models were to relax some of the [assumptions of the Mack method](#cl-ass):

* [LCL](#meyers-lcl): Relaxes the first assumption where Mack treats loss to date as a fixed level parameters

* [CCL](#meyers-ccl): Builds on top of LCL and allows for AY correlations, which relaxes the 2^nd^ assumption of Mack

Prior distributions

* Paper uses diffuse prior for the most part since the author doesn't have direct knowledge of the business

* Given more direct knowledge of the underlying business, we can specify more restrictive priors for $\{\alpha_w\}$ and $logelr$

**Bayesian Models**:

* [Leveled Chain-Ladder](#meyers-lcl) (LCL): Add variability to the row parameter

* [Correlated Chain-Ladder](#meyers-ccl) (CCL): Add AY correlation $\rho$

* [Leveled Incremental Trend](#meyers-lit) (LIT): Use skewed distribution and CY trend $\tau$

* [Correlated Incremental Trend](#meyers-cit) (CIT): LIT with added AY correlation $\rho$

* [Changing Settlement Rate](#meyers-csr) (CSR): LCL with speed up claims closure $\gamma$

```{r meyers-models-overview, echo = FALSE, out.width='100%', fig.show='hold', fig.cap='Overview of models'}
knitr::include_graphics('figures/Exam-7-Notes-9.png')
```

**Non-Bayesian Models**:

* [Mack](#meyers-mack): See [Mack-1994](#cl-ass)

* [England & Verall ODP](#meyers-odp): See [Shapland](#odp-glm-para), but doesn't have the [residual adjustments](#odp-res)

```{remark}
Non-Bayesian Models

* Mack is the only one that does not have a base form of $\mu_{wd} = \alpha_w + \beta_d$

* ODP is the England & Verall Bootstrap
```

## Non-Bayesian Model

This paper doesn't go into much details about them, they are mostly presented to demonstrate their shortcomings and as a motivation to develop the Bayesian methods

### Mack Model {#meyers-mack}

See procedure from Mack-1994 on the [log-normal CI](#mack-CI-methods)

Recall the [Chainladder assumptions](#cl-ass)

**Incurred**

Light on both tail $\Rightarrow$ Does not have enough variability in it's predicted distribution

**Paid**

Similar to ODP, **biased high** on personal auto and light left tail on WC

### ODP Bootstrap {#meyers-odp}

England & Verall ODP forecasts log incremental losses $\Rightarrow$ Only suitable for paid losses

* Can handle occasional negative losses as long as the $\sum$ column is positive

Same procedure as Shapland paper but without all the [residual adjustments](#odp-res)

Overall shows **biased high**

## Bayesian Models (Cumulative)

***Inputs***

1. **Prior distribution** is needed for each parameters (similar to [Verall](#Verall-intro))

    * **Wide** priors (diffuse)

    * **Narrow** priors: Use expert knowledge in selecting mean and variance of the parameters

2. **Parameters**:

    * $\alpha_w$: row parameters
    
    * $\beta_d$: column parameters
    
    * $\sigma_d$: variance parameters (mostly constant across columns)
    
    * $\tau$: trend
    
    * $\gamma$: change in closure rate
    
3. **Data**: Paid or incurred Triangle

***Output***

* The **posterior distribution** of the **parameters** is expressed as *simulated outputs* (not closed form distribution)

### Leveled Chain Ladder {#meyers-lcl}

**Data**: Cumulative incurred

***Model Specification***

$C_{wd}$ has a **lognormal distribution** with log means $\mu_{wd}$ and log standard deviation $\sigma_d$

\begin{equation}
  C_{wd} \sim \ln \mathcal{N}(\mu_{wd} , \sigma_d)
  (\#eq:meyers-loss-dist-lcl)
\end{equation}

\begin{equation}
  \mu_{wd} = \alpha_w + \beta_d
  (\#eq:meyers-lcl-mean)
\end{equation}

```{remark}
$\alpha_w$

* Is a random variable

* Not value on the diagonal (incurred to date)

* Model select an $\alpha_w$ for each instance of the simulation based on wide priors

* Main feature of this model for adding variability

* $e^{\alpha}$ is sort of the ultimate for the AY
```

```{remark}
$\beta_d$

* $\beta_{10} = 0$ so we have 100% at 10 years and not overdetermining the model

* $e^{\beta_d} < 100\%$ most of the time (i.e. $\beta_d < 0$), represents % paid (incurred) to date
```

```{remark}
$\sigma_d$

* Subject to the following constraints:

\begin{equation}
  \sigma_1 > \sigma_2 > \cdots > \sigma_{10}
  (\#eq:meyers-sigm-dist-lcl)
\end{equation}

* Highest variability @ early ages

* Variance only varies by column ($d$) (not by AYs)

* Since as $d$ increases there are fewer claims that are open and subject to random outcomes
```

**Priors** for $\{\alpha_w\}$, $\{\sigma_d\}$, and $\{\beta_d\}$

* *Wide prior* distribution (all you need to know for exam, below is just FYI)

* Each $\alpha_w \sim \mathcal{N}(\ln(Premium_w) + logelr, \sqrt{10})$

    * $logelr \sim U(-1,0.5)$
    
    * JAGS expression for a normal distribution uses a precision parameter equal to the reciprocal of the variance $\Rightarrow$ $\sqrt{10}$ corresponds to a low precision of 0.1
    
* Each $\beta_d \sim U(-5,5)$ for $d<10$

* Each $\sigma_d = \sum_{i=d}^{10} a_i$ where $a_i \sim U(0,1)$

***Test Results***

* Can compare the variability (s.d.) with Mack by plotting the log(s.d.) of the 2 models

* Model still does not capture the tail appropriately

### Correlated Chain-Ladder {#meyers-ccl}

Build upon the [Leveled Chain-Ladder](#meyers-lcl) by adding $\rho$ to create correlation of losses in one AY and the **previous** AY

**Data**: Cumulative incurred & paid

***Model Specification***

$C_{wd}$ follows a lognormal distribution similar to CCL \@ref(eq:meyers-loss-dist-lcl), but with log means:

\begin{equation}
\mu_{wd} =
\begin{cases}
\alpha_1 + \beta_d & \text{if } w = 1\\
\alpha_w + \beta_d + \rho \cdot \left[ \mathrm{ln}\left(C_{w-1, d}\right) - \mu_{w-1,d} \right] & \text{if } w > 1\\
\end{cases}
(\#eq:meyers-ccl-mean)
\end{equation}

```{remark}
$\rho \cdot \left[ \mathrm{ln}\left(C_{w-1, d}\right) - \mu_{w-1,d} \right]$

* If parameters $\{\alpha_w\}$, $\{\alpha_d\}$ and $\rho$ are given:

    $\rho$ is the correlation coefficient between $\ln(C_{w-1,d})$ and $\ln(C_{wd})$

* $\rho$ is applied to the difference between the log of actual losses and the log mean of the expected loss from the prior AY

* Higher losses in one row $\Rightarrow$ higher expected losses in the following row

* The correlation $\rho$ here is what drives the additional variability

* Model reduces to LCL when $\rho = 0$
```

**Priors** for $\{\alpha_w\}$, $\{\sigma_d\}$, $\{\beta_d\}$ and $\rho$

* Prior is still wide priors

* $\{\alpha_w\}$, $\{\sigma_d\}$, $\{\beta_d\}$ has the same distribution as in LCL

* $\rho \sim U(-1 ,1)$, the full permissible range

***Test Results***

**Incurred**

Results and K-S test show that this model is sufficient

**Paid**

Worst than ODP and Mack, biased high for all lines

#### Predictive Distribution Simulation Process Example

```{block, type='rmdnote'}
This section is still work in progress, and it is not important for the exam
```

Predictive distribution of outcomes is a mixed distribution

* Mixing is specified by the posterior distribution of parameters

Below is a summary for the CCL R script provided by Meyers

Predictive distribution for $\sum_{w=1}^{10} C_{w,10}$ (ultimate loss for all AYs @ age 10) is generated by a simulation

For each of parameter set $\{\alpha_w\}$, $\{\sigma_d\}$, $\{\beta_d\}$ and $\{\rho\}$, start with the given $C_{1,10}$ and calculate the mean $\mu_{2,10}$. Then simulate $C_{2,10}$ from a lognormal distribution with log mean $\mu_{2,10}$ and log standard deviation $\sigma_{10}$

Similarly, use the result of this simulation to simulate $C_{2,10},...C_{10,10}$ Then form the sum $C_{1,10} + \sum_{w=2}^{10} C_{w,10}$

### Changing Settlement Rate {#meyers-csr}

Based on LCL with $\gamma$ that allows for speed up in claim payments (likely from claims being reported and settled faster due to technology)

**Data**: Cumulative paid

***Model Specification***

$C_{wd}$ follows a lognormal distribution similar to CCL \@ref(eq:meyers-loss-dist-lcl), but with log means:

\begin{equation}
\mu_{wd} = \alpha_w + \left[ \beta_d \cdot (1-\gamma)^{w-1}\right]
(\#eq:meyers-csr-mean)
\end{equation}

```{remark}
$\gamma$

* $\gamma >0$ reflects increase in payment speed as $(1-\gamma)^{w-1} < 1$

    * A positive $\gamma$ will cause $\beta_d \cdot (1 - \gamma)^{w-1}$ to increase with $w$ $\Rightarrow$ indicate speedup in claim settlement
    
    * Negative $\gamma$ will indicate a slow down in claim settlement rate

* $\gamma$ has less impact further out in the tail as there are less payments happening out there

* Model fits one $\gamma$ for the whole triangle
```

**Priors** for $\{\alpha_w\}$, $\{\sigma_d\}$, $\{\beta_d\}$ and $\rho$

* *Wide prior* similar to LCL and CCL

* $\gamma \sim \mathcal{N}(0, 0.025)$

***Test Results***

Overall fits well, slightly biased high on Personal Auto but is a big improvement over the other models

## Skewed Distribution

**Motivations**:  
Incremental data has the following properties:

* Skewed right

* Occasionally negative

### Skewed Normal Distribution

From FrÃ¼hwirth-Schnatter and Pyne (2000)

\begin{equation}
X = \mu + (\omega \cdot Z) \cdot \delta + (\omega \cdot \varepsilon) \cdot \sqrt{1 - \delta^2}
(\#eq:meyers-skewed-normal)
\end{equation}

* $\varepsilon \sim \mathcal{N}(0,1)$

* $Z \sim Truncated \: Normal_{[0,\infty]} (0,1)$

* $\mu$ is the location parameter

* $\omega$ is the scale parameter, with $\omega >0$

* $\delta$ is the shape parameter, with $\delta \in (-1,1)$

    Also weight between $\varepsilon$ and $Z$

Max skewness = 0.995 from the truncated normal

$\therefore$ **Not used** by the Meyers as it is not skewed enough

### Mixed Lognormal-Normal

Mixed ln-n distribution with parameters $\delta$, $\mu$ and $\sigma$

\begin{equation}
\begin{split}
  X \sim \mathcal{N}(Z, \delta) \\
  Z \sim \ln \mathcal{N}(\mu, \sigma) \\
\end{split}
(\#eq:meyers-mix-log-norm-norm)
\end{equation}

**Pros**

* Can create distribution more skew than the skewed normal

* Can also have negative values

## Bayesian Models (Incremental)

Model applies CY trend and therefore uses **incremental** data $I_{wd}$

* Correlated Incremental Trend Model

* Level Incremental Trend Model

### Correlated Incremental Trend {#meyers-cit}

$I_{wd}$ has a mixed ln-n distribution \@ref(eq:meyers-mix-log-norm-norm)

\begin{equation}
\begin{array}.
I_{wd} \sim
\begin{cases}
  \mathcal{N}(Z_{1,d}, \delta) & \text{if }w = 1 \\
  \mathcal{N}(Z_{w,d} + \rho \cdot (I_{w-1,d} - Z_{w-1,d}) \cdot e^{\tau}, \delta) & \text{if } w > 1\\
\end{cases} \\
Z_{w,d} \sim \ln \mathcal{N}(\mu_{w,d}, \sigma_d) \\
\end{array}
(\#eq:meyers-CIT-distribution)
\end{equation}

With log-normal mean:

\begin{equation}
\mu_{wd} = \alpha_w + \beta_d + \tau \cdot (w+d-1) \\
(\#eq:meyers-CIT-mean)
\end{equation}

```{remark}
$\tau$

* CY trend parameter

* Note the $\tau$ is applied additively in the "log" space \@ref(eq:meyers-CIT-mean), therefore in the autocorrelation step \@ref(eq:meyers-CIT-distribution) it is applied by $e^{\tau}$ 
```

```{remark}
$\sigma_d$

* Subject to the following constaints **different** from LCL and CCL

\begin{equation}
  \sigma_1 < \sigma_2 < \cdots < \sigma_{10}
  (\#eq:meyers-sigm-dist-cit)
\end{equation}

* Since we're looking at *incremental* data, smaller less volatile claims should be settled early
```

```{remark}
$\rho$

* Include correlation between AYs similar to CCL method

* $\rho$ is the coefficient of correlation between $I_{w-1,d}$ and $I_{w,d}$

    * Note $\rho$ is applied outside of the "log" space here to the **incremental** loss (not log)
    
    * Recall for CCL we apply $\rho$ to the $\ln(C_{w-1,d})$ but we can't do that here due to the chance of negative incremental losses
```

**Priors** for $\{\alpha_w\}$, $\{\sigma_d\}$, $\{\beta_d\}$, $\rho$, and $\tau$

* *Disperse* priors similar to LCL and CCL unless mentioned below

* Each $\beta_d \sim \begin{cases} U(0, 10) & \text{if } d \leq 4 \\ U(0, \beta_{d-1}) &\text{if } d > 4 \\ \end{cases}$

    * This assure $\beta_d$ decreases for $d>4$

* $\tau \sim \mathcal{N}(0,3.2\%)$, which correspond to a precision parameter by JAGs of 1000

    * Without restriction it was forecasting very negative trend which is offset by higher $\alpha$ and $\beta$
    
    * Meyers expected this to be predominantly negative since the other paid method we've tried so far were biased high

* Each $\sigma_d \sim \begin{cases} U(0,0.5) & \text{if } d = 1 \\ U(\sigma^2_{d-1},\sigma^2_{d-1} +0.1) & \text{else} \\ \end{cases}$

    * Limit the speed $\sigma_d$ can increase, very high $\sigma_d$ can lead to unreasonably high simulate results

* $\delta \sim U(0, \text{Avg Premium})$

***Steps for the method:***

1) Uncorrelated log mean of each cell with CY trend  
$\mu_{wd} = \alpha_w + \beta_d + \tau \cdot(w+d-1)$

2) Draw $Z_{wd} \sim Lognormal(\mu_{wd},\sigma_d)$

    * $\sigma_1 > \sigma_2 > \cdots > \sigma_{10}$
    
    * Smaller less volatile claims should be settled early

3) $\tilde{I}_{wd} \sim Normal(Z_{wd},\delta)$

4) Add correlation between AYs for rows after the first  
$\tilde{I}_{wd} \sim Normal(Z_{wd} + \rho \cdot (\tilde{I}_{w-1,d} - Z_{w-1,d})\cdot e^{\tau},\delta)$

***Test Results***

* Losses not much smaller than CCL while we would like it to be much smaller as CCL was biased high

* $\rho$ is lower than from CCL

* Strong negative correlation between trend $\tau$ and level parameters $\alpha_w + \beta_d$

    * With small data set it is hard for the model to distinguish the AY level + development vs trend
    
    * Based on scatter plot of $\tau$ vs $\alpha_w + \beta_d$ for several $d$ and $w=6$
    
* Average $\tau$ is negative

* Model showed no improvement over Mack or ODP

### Leveled Incremental Trend {#meyers-lit}

Same as CIT but with $\rho = 0$

Results similar to CIT with lower standard deviation

## Process, Parameter, and Model Risk

**Process & Parameter Risk**

$$\underbrace{\text{Variance}}_{\mathrm{Var}(X)} = \underbrace{\mathrm{E}[\text{Process Variance}]}_{\mathrm{E}_{\theta}[\mathrm{Var}[X|\theta]]}+\underbrace{\mathrm{Var}[\text{Hypothetical Mean}]}_{\mathrm{Var}_{\theta}[\mathrm{E}[X|\theta]]}$$

* Process risk: average variance of the outcomes from the expected result

* Parameter risk: variance due to the many possible parameters in the posterior distribution

    * Similar to the idea of "range of reasonable estimates"

* Typically the parameter risk is much larger than process risk

    * e.g. Total Risk = $\mathrm{Var}\left[ \sum \limits_{w=1}^{10} C_{w,10} \right]$
    
        Parameter Risk = $\mathrm{Var}_{\theta}\left[\mathrm{E}\left[\sum \limits_{w=1}^{10} C_{w,10}|\theta\right]\right] = \mathrm{Var}\left[ \sum \limits_{w=1}^{10} e^{\mu_{w,10} + \frac{\sigma^2_{10}}{2}} \right]$

**Model Risk**: Risk of not selecting the right model

* If possible models are "know unknowns", we can turn the model risk into parameter risk

    * Formulate a model as a weighted average of the candidate models with the weights as parameters
    
    * If posterior distribution of the weights assigned to each model has significant variability, this indicates of model risk
    
    * And in this case, just a special case of parameter risk
    
* If we have a very large dataset to run this model on, the parameter risk will shrink towards 0 and any remaining risk such as model risk will be interpreted as process risk

    * This is mostly a academia thought experiment as most aggregated loss triangles are small datasets
    
    * This serves to illustrate the theoretical difficulties that occur when we try to work with parameter/process/model classification of risk
    
    $\therefore$ Should focus on the total risk

## Conclusion

Goal of the paper was to test the predictive accuracy of various models, both mean and distribution of outcomes

* Not on the reserve estimate for individual insurers

Bayesian MCMC models can be developed to overcome shortcomings in existing models (e.g. how LCL and CCL loosens some of Mack Chainladder's key assumptions)

### Results Summary

**Incurred Data**

Mack understates variability as it assumes AYs are independent

CCL introduces AY correlation and does relatively well

**Paid Data**

Mack and ODP were biased high as well as CCL

There were change in environment that is not captured

* Calendar year trend: LIT and CIT still biased high

* CSR: significantly less bias than LIT and CIT (except for PA still failed)

Mack and ODP did better than CCL, LIT and CIT

### Final Comments

Results were for specific annual statement year 1997

* Possible the speed up of claims settlement was specific to the period $\Rightarrow$ CSR could potentially useless for another year

Could use more narrow priors to incorporate knowledge of insurer's business operation and obtain superior results

## Appendix B: Intro to Bayesian MCMC Models

```{block, type='rmdtip'}
Just additional background information, not part of exam syllabus
```

```{definition, markov-chain-meyers}
Markov Chain

Random process where the transition to the next state depends only on its current state and not on prior states

A Markov chain $X_t$ for $t=1,2,...$ is a sequence of vectors satisfying the property that

$$\Pr(X_{t+1} = x \mid X_1 = x_1, X_2 = x_2,...,X_t = x_t) = \Pr(X_{t+1} \mid X_t = x_t)$$

```

Key properties of Markov chain for Bayesian MCMC

* Ergodic class of Markov chain, for which vectors $\{X_t\}$ approaches a limiting distribution

    As $T$ increases, the distribution of $\{X_t\}$ for all $t>T$ approaches a unique limiting distribution
    
* Markov chains used in Bayesian MCMC (e.g. Metropolis Hastings algorithm) are members of Ergodic class

Let $x$ be a vector of observations and let $y$ be a vector of parameters in a model

* In Bayesian MCMC, the Markov chain is defined in terms of the prior distribution $p(y)$ and the conditional distribution $f(x \mid y)$

* The limiting distribution is the posterior distribution $f(y \mid x)$

* If we let the chain run *long enough*, the chain will randomly visit all states with frequency that is proportional to their posterior probabilities

The operative phrase above is **long enough**, which means in practice:

1. Develop algorithm for obtaining a chain that is *long enough* as quickly as possible

2. Develop criteria for being *long enough*

### How Bayesian MCMC works in practice

1. User specifies $p(y)$ and $f(x \mid y)$

2. User selects a starting vector $x_1$

    Using computer simulation, runs the Markov chain through a sufficiently large number, $t_1$, of iterations
    
    This first phase of the simulation is called the "adaptive" phase
    
    * The algorithm is automatically modified to increase its efficiency
    
    * See below on the [Metropolis-Hasting alogrithm](#meyers-mh-algo)
    
3. User runs an additional $t_2$ iterations

    This is the "burn-in" phase
    
    $t_2$ is selected to be high enough so that a sample taken from subsequent $t_3$ periods represents the posterior distribution
    
4. User then runs an additional $t_3$ iterations and then takes a sample, $\{ x_t \}$ from the $(t_2 + 1)$^th^ step to the $(t_2 + t_3)$^th^ step to represent the posterior distribution $f(y \mid x)$

5. From the sample, we can constructs various statistics of interest that are relevant to the problem addressed by the analysis

### Metropolis-Hastings Algorithm {#meyers-mh-algo}

Most common algorithms for generating Bayesian Markov chains are variants of the Metropolis-Hastings algorithm

*Given*: $p(y)$ and $f(x \mid y)$

The algorithm introduces a 3^rd^ distribution $J(y_t \mid y_{t-1})$:

* "Proposal" or "jumping" distribution

Given a parameter vector $y_{t-1}$ the algorithm generates a Markov chain by the following steps:

1. Select a candidate value $y^*$, at random from the proposal distribution $J(y_t \mid y_{t-1})$

2. Computer the ratio

$$R \equiv R_1 \times R_2 = \dfrac{f(x \mid y^*) \cdot p(y^*)}{f(x \mid y_{t-1}) \cdot p(y_{t-1})} \times \dfrac{J(y_{t-1} \mid y^*)}{J(y^* \mid y_{t-1})}$$

3. Select $U$ at random from $U(0,1)$ distribution

4. If $U < R$ then set $y_t = y^*$, else $y_t = y_{t-1}$

```{remark}


* $R_1$ represents the ratio of the posterior probability of the proposal $y^*$ to the posterior probability of $y_{t-1}$

* The higher the value of $R_1$, the more likely will be accepted into the chain

* Regardless of how the proposal density distribution is chosen, the distribution of $y_t$ can be regarded as a sample from the posterior distribution, after a suitable burn-in period
```

**Example**

* We can look at *trace plots* to look at the value of the parameter as the chain progresses

* More on how it might break in the paper...

* The key is to be able to scale the proposal distribution to minimize auto correlation

    * This is difficult with many parameters

**Minimizing autocorrelation** in Metropolis-Hasting

* A good statistics to look at is the acceptance rate of $y^*$

* 50% is near optimal for a one parameter model and the rate decreases to about 25% as we increase the number of parameters in the model

* There are methods to automatically adjust the proposal density function in Metropolis-Hastins

* All these have been mechanized in software like JAGS and STAN

* The phase of generating the Markov chain where the proposal density function is optimized is called the "adaptive" state (as discussed above)

As models become more complex, adaptive MCMC may not be good enough to eliminate the auto correlation

* Theory on Markov chain convergence will still hold but there is no guarantee on how fast it will converge

* If there is significance auto correlation after the best scaling effort, the next best practice is to increase $t_3$ until there are sufficient number of ups and downs in the trace plot and then take a sample of the $(t_1 + t_2 +1)$^th^ to $(t_1 + t_2 + t_3)$^th^ iterations

    * This process is known as "thinning"

### Usecase Example for Actuaries

Look at how the posterior distribution of $\mu$ might be of interest

Consider a fitted lognormal distribution for a set of claims to determine the cost of an XS layer

Given $\mu$ and $\sigma$ of the lognormal we can determine the cost of an XS layer (see Klugman, Panjer, and Willmot and the actuar package)

As the posterior distribution of $\mu$ reflects the parameter risk in the model, it is also possible to reflect the parameter risk in the expected cost of a layer by calculating the expected cost of the layer for each $\mu$ in the simulated posterior distribution

It is possible to simulate an actual outcome of a loss $X$ in a layer given $\mu$ in the posterior distribution

The distribution $X$ calculated this way reflects both the parameter risk and process risk in the model

### Bayesian interence Using Gibbs Sampling (BUGS)

WinBUGS, JAGS are examples of software using Gibbs sampling

Sample work process:

1. Read data into R and call JAGS script with R package "runjags"

2. Fetch the sample of the posterior back into R to calculate statistics of interest

JAGS perform the Metropolis-Hasting algorithm we described above

It also has a number of convergence diagnostics

## Past Exam Questions

**TIA Exercise**

* $\star$ TIA 1: Perform KS test on given percentile

* TIA 2: read p-p plot and structure correlated AY model

* TIA 3: Skewed normal limitation

* TIA 4: Describe CIT model

* TIA 5: Read p-p plot

* $\star$ TIA 6: read p-p plot and how to address the issues

**Past Exam**

* 2016 #15:

    a. KS Test with given percentile
    
    b. Reason why model can not predict
    
    c. How to improve on the Mack model

### Question Highlights

n/a